---
title: "Project Feedback"
author: "Priyank Thakkar"
date: "18/11/2021"
output: word_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## INTRODUCTION

This data set is all about the students who graduate from the Universities of USA. I am curious to know that does your Major in your study matter or not for your economic success. What is the general trend right now. And, what are the steps we can take before selecting the Major to boost your odds for the same. And mostly what is the share of the women in the different categories. 

## DATA 

This data is from the GitHub repository, from Fivethirtyeight.com.  All data is from American Community Survey 2010-2012 Public Use Micro data series. All Three files in the repository contains basic earnings and labor force information. “recent-grads.csv” contains a more detailed breakdown, including by sex and by the type of job they got. “grad-students.csv” contains details on graduate school attendees. 
Here, we have used the data from the file of “recent-grades.csv”. And here is the bifurcation of all the Headers and their Descriptions. Our data contains 174x21 size of entries.

Where, we can see that 174 are rows, they represent distinct 174 majors, from all Major Categories available in the data. Moreover, all 21 columns and their representation are below.

**Header**		          **Description** 
Rank:                   Rank by median earnings
Major_code:             Major code
Major:                  Major description
Major_category:         Category of major from Carnevale et al
Total:                  Total number of people with major
Sample_size:            Sample size (unweighted) of full-time, year-round ONLY 
Men:                    Male graduates
Women:                  Female graduates
ShareWomen:             Women as share of total
Employed:               Number employed
Full_time:              Employed 35 hours or more
Part_time:              Employed less than 35 hours
Full_time_year_round:   Employed at least 50 weeks and at least 35 hours
Unemployed:             Number unemployed 
Unemployment_rate:      Unemployed / (Unemployed + Employed)
Median:                 Median earnings of full-time, year-round workers (Normalized)
P25th:                  25th percentile of earnings
P75th:                  75th percentile of earnings
College_jobs:           Number with job requiring a college degree
Non_college_jobs:       Number with job not requiring a college degree
Low_wage_Jobs:          Number in low-wage service jobs



For our variable study, we may need all the variables later on for more explorations. But, here are few of the variables that we can focus on.
 - Total, Men, Women
 - ShareWomen
 - Mean 
 - Employed
 - Unemployed
 - Unemployment_rate
 - College_jobs
 - Non_college_jobs
 - Low_wage_jobs
 
 ## Exploratory Data Analysis

```{r EDA}
library(tidyverse)
library(pls)

setwd("D:\\SJSU_HW\\GitHubSJSU\\RStudio_Learning\\Data_set")
data_grade <- read.csv("recent_grads.csv")
recent_grade <- as_tibble(data_grade)
is_tibble(recent_grade)
str(recent_grade)
```

```{r Missing Value}
# NA values
new_recent_grade <- drop_na(recent_grade)

batch <- select(new_recent_grade, Median, Employed, Unemployed, Full_time, College_jobs)
is_tibble(batch)
str(batch)
```
```{r cov for swiss}
# Calculate the Covariance
cov <- cov(batch)
cov

# Calculate the Correlation 
cor <- cor(batch)
cor

# Calculate the Scaled Covariance = correlation
scaled <- scale(batch)
ScaleCov <- cov(scaled)
ScaleCov
```
```{r Eigen values for the Swiss as covariance}
eigenCov <-  eigen(cov)
eigenCov
```
```{r Eigen values for the Swiss as corelation}
eigenCor<-  eigen(cor)
eigenCor
```
Now, we will calculate the Percent Variance, and that will give us the information about, what proportion of total variance is explained by the First, second and till the end of principal component. 

We can calculate Cumulative percent variance just to see that how many columns represents major portion of the data information.

```{r PVE}
# Percent Variance
PV <- eigenCor$values/sum(eigenCor$values)
PV

# Cumulative Percent Variance
cumsum(PV)
```

Now, using the Graphs we will can see that how many variables we need to represent the data and what are the variables we can reduce. 

```{r plots for swiss}
SwCorPlot <- qplot(c(1:5),PV) +
  geom_line()+
  geom_point(shape = 20,colour = "red", fill = NA , size = 2, stroke = 1 ) +
  xlab("Principal Component") +
  ylab("Percent Variace") +
  ggtitle("Correlation Plot of Grad Students") +
  scale_y_continuous(labels = scales::percent)+
  theme(plot.title = element_text(size = rel(2))) +
  theme(panel.grid.major = element_line(colour = "black")) +
  theme(panel.border = element_rect(linetype = "dashed", fill = NA)) +
  theme(axis.ticks = element_line(size = 2)) +
  
  theme(
    axis.ticks.length.y = unit(.25, "cm"),
    axis.ticks.length.x = unit(-.25, "cm"),
    axis.text.x = element_text(margin = margin(t = .3, unit = "cm"))
  ) 
SwCorPlot

SwCumCorPlot <- qplot(c(1:5),cumsum(PV)) +
  geom_line()+
  geom_point(shape = 20,colour = "red", fill = NA , size = 2, stroke = 1 ) +
  xlab("Principal Component") +
  ylab("Percent Variace") +
  ggtitle("Cumulative Correlation Plot of Grad Students") +
  scale_y_continuous(labels = scales::percent) +
  theme(plot.title = element_text(size = rel(2))) +
  theme(panel.grid.major = element_line(colour = "black")) +
  theme(panel.border = element_rect(linetype = "dashed", fill = NA)) +
  theme(axis.ticks = element_line(size = 2)) +
  
  theme(
    axis.ticks.length.y = unit(.25, "cm"),
    axis.ticks.length.x = unit(-.25, "cm"),
    axis.text.x = element_text(margin = margin(t = .3, unit = "cm"))
  ) 
SwCumCorPlot
```
Now, We will count the Principal Components Score. 

The sample principal components are defined as those linear combinations which have maximum sample variance. If we project the 47 data points onto the first eigen vectors, the projected values are called the first principal component.
```{r axis for swiss}
selectedEigenValues <- eigenCor$vectors[,1:3]
colnames(selectedEigenValues) = c("PC1", "PC2", "PC3")
row.names(selectedEigenValues) = colnames(batch)
selectedEigenValues

```
```{r Principal componenet scores for swiss}
PC1 <- as.matrix(scaled) %*% selectedEigenValues[,1]
PC2 <- as.matrix(scaled) %*% selectedEigenValues[,2]
PC3 <- as.matrix(scaled) %*% selectedEigenValues[,3]

PC <- data.frame(grade_student = row.names(batch), PC1,PC2,PC3)
head(PC)
```

```{r Visualization of the Principal components selected}
scatterplot3d(PC[,1:3], angle = 75, pch = 16,
              main = "Grade Student - 3D PCA Graph",
              xlab = "PCA 1",
              ylab = "PCA 2",
              zlab = "PCA 3",
              color = "Blue",
              type = "h"
              )
```
```{r scatter plot}
ggplot(PC, aes(PC1,PC2))+
  modelr::geom_ref_line(h=0) +
  modelr::geom_ref_line(v=0) +
  geom_text(aes(label = grade_student),size =3) +
  xlab("PCA 1") +
  ylab("PCA 2") +
  ggtitle("Grade Student - PCA Graph")
```

##################################################


```{r }

```

```{r PCA from function}
results <- prcomp(batch, scale = TRUE)
results
```
```{r biplot 1}
biplot(results)
```

```{r biplot 2}
biplot(results, scale = 0,  expand=3, xlim=c(-3.0, 12.0), ylim=c(-2.0, 2.0))
```


## 2. Principal Components Regression
```{r seed}
#make this example reproducible
set.seed(1)
```

Step 2: Fit PCR Model

```{r model}
model <- pcr(Median~Employed+Unemployed+Full_time+College_jobs, 
             data=batch, 
             scale=TRUE,
             validation="CV")

```

Step 3: Choose the Number of Principal Components
```{r summary of model}
#view summary of model fitting
summary(model)
```
There are two tables of interest in the output:

1. VALIDATION: RMSEP

This table tells us the test RMSE calculated by the k-fold cross validation. We can see the following:

    If we only use the intercept term in the model, the test RMSE is 69.66.
    If we add in the first principal component, the test RMSE drops to 44.56.
    If we add in the second principal component, the test RMSE drops to 35.64.

We can see that adding additional principal components actually leads to an increase in test RMSE. Thus, it appears that it would be optimal to only use two principal components in the final model.

2. TRAINING: % variance explained

This table tells us the percentage of the variance in the response variable explained by the principal components. We can see the following:

    By using just the first principal component, we can explain 69.83% of the variation in the response variable.
    By adding in the second principal component, we can explain 89.35% of the variation in the response variable.

Note that we’ll always be able to explain more variance by using more principal components, but we can see that adding in more than two principal components doesn’t actually increase the percentage of explained variance by much.

We can also visualize the test RMSE (along with the test MSE and R-squared) based on the number of principal components by using the validationplot() function. 

```{r model visualization }
#visualize cross-validation plots
validationplot(model)
validationplot(model, val.type="MSEP")
validationplot(model, val.type="R2")
```
In each plot we can see that the model fit improves by adding in two principal components, yet it tends to get worse when we add more principal components.

Thus, the optimal model includes just the first two principal components.

#Step 4: Use the Final Model to Make Predictions

We can use the final PCR model with two principal components to make predictions on new observations.

The following code shows how to split the original dataset into a training and testing set and use the PCR model with two principal components to make predictions on the testing set.

```{r plot}
predplot(model)
coefplot(model)
```
```{r train model}
#define training and testing sets
train <- batch[1:130, c("Median", "Employed", "Unemployed", "Full_time", "College_jobs")]
y_test <- batch[131:nrow(batch), c("Median")]
test <- batch[131:nrow(batch), c("Employed", "Unemployed", "Full_time", "College_jobs")]
```

```{r prediction}
#use model to make predictions on a test set
modelT <-pcr(Median~Employed+Unemployed+Full_time+College_jobs, 
             data=train, 
             scale=TRUE,
             validation="CV")
pcr_pred <- predict(modelT, test, ncomp=2)
```

```{r RMSE}
#calculate RMSE
sq = (pcr_pred - y_test)^2 
sqrt(mean(sq$Median)) 
```
```{r predict data frame}
nd = with(train, data.frame(Employed=1976, Unemployed=37, Full_time=1800, College_jobs=1534))
nd$pred = predict(modelT, newdata=nd, type="response")
nd
```
```
```{r }

```
```{r }

```
```{r }

```
```{r }

```
```{r }

```
```{r }

```
```{r }

```
```{r }

```
```{r }

```